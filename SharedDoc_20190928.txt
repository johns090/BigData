SHOW DATABASES;

CREATE DATABASE tempdb;

hadoop fs -ls /user/hive/warehouse

create table t1(c1 int, c2 int);

hadoop fs -ls /user/hive/warehouse/tempdb.db 	

Use tempdb;

create table t1(c1 int, c2 int);

hadoop fs -ls /user/hive/warehouse/tempdb.db/t1

insert into t1 values (1,1);

hadoop fs -ls /user/hive/warehouse/tempdb.db/t1

hadoop fs -cat /user/hive/warehouse/tempdb.db/t1/000000_0

hadoop fs -copyToLocal /user/hive/warehouse/tempdb.db/t1/000000_0 ./

cp 000000_0 000000_1 

hadoop fs -copyFromLocal 000000_1 /user/hive/warehouse/tempdb.db/t1/

select * from t1

hadoop fs -mkdir /data

create table mydb.t1 (c1 int, c2 int) row format delimited fields terminated by ',';

hadoop fs -cat /data/mydb/t3/*

FIX TABLE

create table t4 as select * from t3 limit 0;

load data inpath '/data/mydb/t3/' into table t4;

insert into t3 select * from t4 where c2 is not null;

create table t5 like t3;

load data inpath '/data/mydb/t4' into table t5;

insert into t3 select * from t5 where c2 is not null;

Create External Table

create external table t3 like t1;

UPDATE/DELETE

insert overwrite table t1 select case when c2=3 then 'hi hi' else c1 end as c1, c2 from t1;

insert overwrite table t1 select * from t1 where c2 != 3;

hadoop fs -getmerge /retail/people/ ./ppl


sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username retail_dba --password cloudera --table t1 --m 1 --target-dir /retail/sqooptest

sqoop export --connect jdbc:mysql://localhost:3306/retail_db --username retail_dba --password cloudera  --table t1 --fields-terminated-by ',' --export-dir /retail/sqooptest



